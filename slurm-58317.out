/var/spool/slurmd/job58317/slurm_script: line 11: cd: SSL_VAN: No such file or directory

----------------------------------------------------------------------------
  cuda:
----------------------------------------------------------------------------
     Versions:
        cuda/10.2
        cuda/11.0
        cuda/11.2
        cuda/11.3
        cuda/11.4
        cuda/11.5
        cuda/11.6
        cuda/11.7
        cuda/11.8
        cuda/12.0

----------------------------------------------------------------------------
  For detailed information about a specific "cuda" package (including how to load the modules) use the module's full name.
  Note that names that have a trailing (E) are extensions provided by other modules.
  For example:

     $ module spider cuda/12.0
----------------------------------------------------------------------------

 


CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/karimimonsefi.1/SSL_VAN/BRATS21/main.py:104: UserWarning: Found total gpus 2
  warnings.warn("Found total gpus {}".format(args.ngpus_per_node))
/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/monai/transforms/post/dictionary.py:196: UserWarning: `threshold_values=True/False` is deprecated, please use `threshold=value` instead.
  warnings.warn("`threshold_values=True/False` is deprecated, please use `threshold=value` instead.")
/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/monai/transforms/post/dictionary.py:196: UserWarning: `threshold_values=True/False` is deprecated, please use `threshold=value` instead.
  warnings.warn("`threshold_values=True/False` is deprecated, please use `threshold=value` instead.")
/home/karimimonsefi.1/SSL_VAN/BRATS21/main.py:163: UserWarning: 0 gpu 0
  warnings.warn(f"{args.rank} gpu {args.gpu}")
/home/karimimonsefi.1/SSL_VAN/BRATS21/main.py:165: UserWarning: Batch size is: 1 epochs 500
  warnings.warn(f"Batch size is: {args.batch_size} epochs {args.max_epochs}")
/home/karimimonsefi.1/SSL_VAN/BRATS21/main.py:163: UserWarning: 1 gpu 1
  warnings.warn(f"{args.rank} gpu {args.gpu}")
/home/karimimonsefi.1/SSL_VAN/BRATS21/main.py:195: UserWarning: Total parameters count 57746013
  warnings.warn(f"Total parameters count {pytorch_total_params}")
/home/karimimonsefi.1/SSL_VAN/BRATS21/main.py:198: UserWarning: Total args.checkpoint True
  warnings.warn(f"Total args.checkpoint {args.checkpoint}")
/home/karimimonsefi.1/SSL_VAN/BRATS21/main.py:206: UserWarning:  Best url model is 64-128-256-512_3-4-6-3_8-8-4-4_vae_inferer_valid_loader_VANV6GL_2_fold-0__best.pt, final model url is 64-128-256-512_3-4-6-3_8-8-4-4_vae_inferer_valid_loader_VANV6GL_2_fold-0__final.pt
  warnings.warn(f" Best url model is {args.best_model_url}, final model url is {args.final_model_url}")
/home/karimimonsefi.1/SSL_VAN/BRATS21/main.py:195: UserWarning: Total parameters count 57746013
  warnings.warn(f"Total parameters count {pytorch_total_params}")
/home/karimimonsefi.1/SSL_VAN/BRATS21/main.py:198: UserWarning: Total args.checkpoint True
  warnings.warn(f"Total args.checkpoint {args.checkpoint}")
/home/karimimonsefi.1/SSL_VAN/BRATS21/main.py:206: UserWarning:  Best url model is 64-128-256-512_3-4-6-3_8-8-4-4_vae_inferer_valid_loader_VANV6GL_2_fold-0__best.pt, final model url is 64-128-256-512_3-4-6-3_8-8-4-4_vae_inferer_valid_loader_VANV6GL_2_fold-0__final.pt
  warnings.warn(f" Best url model is {args.best_model_url}, final model url is {args.final_model_url}")
/home/karimimonsefi.1/SSL_VAN/BRATS21/main.py:249: UserWarning: => loaded checkpoint 'True' (epoch 16) (bestacc 0.01244897861033678)
  warnings.warn("=> loaded checkpoint '{}' (epoch {}) (bestacc {})".format(
/home/karimimonsefi.1/SSL_VAN/BRATS21/main.py:249: UserWarning: => loaded checkpoint 'True' (epoch 16) (bestacc 0.01244897861033678)
  warnings.warn("=> loaded checkpoint '{}' (epoch {}) (bestacc {})".format(
/home/karimimonsefi.1/SSL_VAN/BRATS21/trainer.py:164: UserWarning: Writing Tensorboard logs to ./runs/BraTS21_new/test_log
  warnings.warn(f"Writing Tensorboard logs to {args.logdir}")
/home/karimimonsefi.1/SSL_VAN/BRATS21/trainer.py:172: UserWarning: 0  Wed Jun  7 05:29:29 2023  Epoch: 16
  warnings.warn(f"{args.rank}  {time.ctime()}  Epoch: {epoch}")
/home/karimimonsefi.1/SSL_VAN/BRATS21/trainer.py:172: UserWarning: 1  Wed Jun  7 05:29:29 2023  Epoch: 16
  warnings.warn(f"{args.rank}  {time.ctime()}  Epoch: {epoch}")
[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Traceback (most recent call last):
  File "/home/karimimonsefi.1/SSL_VAN/BRATS21/main.py", line 286, in <module>
    main()
  File "/home/karimimonsefi.1/SSL_VAN/BRATS21/main.py", line 106, in main
    mp.spawn(main_worker, nprocs=args.ngpus_per_node, args=(args,))
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/karimimonsefi.1/SSL_VAN/BRATS21/main.py", line 263, in main_worker
    accuracy = run_training(
  File "/home/karimimonsefi.1/SSL_VAN/BRATS21/trainer.py", line 174, in run_training
    train_loss = train_epoch(
  File "/home/karimimonsefi.1/SSL_VAN/BRATS21/trainer.py", line 60, in train_epoch
    scaler.step(optimizer)
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 341, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 288, in _maybe_opt_step
    retval = optimizer.step(*args, **kwargs)
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/optim/adamw.py", line 162, in step
    adamw(params_with_grad,
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/optim/adamw.py", line 219, in adamw
    func(params,
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/optim/adamw.py", line 273, in _single_tensor_adamw
    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cpu!

