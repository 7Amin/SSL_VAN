
----------------------------------------------------------------------------
  cuda:
----------------------------------------------------------------------------
     Versions:
        cuda/10.2
        cuda/11.0
        cuda/11.2
        cuda/11.3
        cuda/11.4
        cuda/11.5
        cuda/11.6
        cuda/11.7
        cuda/11.8
        cuda/12.0

----------------------------------------------------------------------------
  For detailed information about a specific "cuda" package (including how to load the modules) use the module's full name.
  Note that names that have a trailing (E) are extensions provided by other modules.
  For example:

     $ module spider cuda/12.0
----------------------------------------------------------------------------

 


CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/karimimonsefi.1/SSL_VAN/pretrain/main.py:109: UserWarning: Found total gpus 2
  warnings.warn("Found total gpus {}".format(args.ngpus_per_node))
/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/karimimonsefi.1/SSL_VAN/pretrain/main.py:135: UserWarning: 0 gpu 0
  warnings.warn(f"{args.rank} gpu {args.gpu}")
/home/karimimonsefi.1/SSL_VAN/pretrain/main.py:135: UserWarning: 1 gpu 1
  warnings.warn(f"{args.rank} gpu {args.gpu}")
/home/karimimonsefi.1/SSL_VAN/pretrain/main.py:137: UserWarning: Batch size is: 2 epochs 15000
  warnings.warn(f"Batch size is: {args.batch_size} epochs {args.max_epochs}")
/home/karimimonsefi.1/SSL_VAN/pretrain/main.py:144: UserWarning: Total parameters count 36285642
  warnings.warn(f"Total parameters count {pytorch_total_params}")
/home/karimimonsefi.1/SSL_VAN/pretrain/main.py:147: UserWarning: Total args.checkpoint True
  warnings.warn(f"Total args.checkpoint {args.checkpoint}")
/home/karimimonsefi.1/SSL_VAN/pretrain/main.py:156: UserWarning:  Best url model is pre_train_64-128-256-512_3-4-6-3_8-8-4-4_vae_inferer_valid_loader_PREVANV4__best.pt, final model url is pre_train_64-128-256-512_3-4-6-3_8-8-4-4_vae_inferer_valid_loader_PREVANV4__final.pt
  warnings.warn(f" Best url model is {args.best_model_url}, final model url is {args.final_model_url}")
/home/karimimonsefi.1/SSL_VAN/pretrain/main.py:144: UserWarning: Total parameters count 36285642
  warnings.warn(f"Total parameters count {pytorch_total_params}")
/home/karimimonsefi.1/SSL_VAN/pretrain/main.py:147: UserWarning: Total args.checkpoint True
  warnings.warn(f"Total args.checkpoint {args.checkpoint}")
/home/karimimonsefi.1/SSL_VAN/pretrain/main.py:156: UserWarning:  Best url model is pre_train_64-128-256-512_3-4-6-3_8-8-4-4_vae_inferer_valid_loader_PREVANV4__best.pt, final model url is pre_train_64-128-256-512_3-4-6-3_8-8-4-4_vae_inferer_valid_loader_PREVANV4__final.pt
  warnings.warn(f" Best url model is {args.best_model_url}, final model url is {args.final_model_url}")
/home/karimimonsefi.1/SSL_VAN/pretrain/training.py:109: UserWarning: Writing Tensorboard logs to ./runs/pre_train_1/test_log
  warnings.warn(f"Writing Tensorboard logs to {args.logdir}")
/home/karimimonsefi.1/SSL_VAN/pretrain/training.py:119: UserWarning: 0  Sun Jun 11 01:23:13 2023  Epoch: 0
  warnings.warn(f"{args.rank}  {time.ctime()}  Epoch: {epoch}")
/home/karimimonsefi.1/SSL_VAN/pretrain/training.py:119: UserWarning: 1  Sun Jun 11 01:23:13 2023  Epoch: 0
  warnings.warn(f"{args.rank}  {time.ctime()}  Epoch: {epoch}")
Dataset 1 LUNA16: OK number of data: 842
Dataset 2 Covid 19: OK number of data: 722
Dataset 3 HNSCC: OK number of data: 954
Dataset 4 TCIA Colon: OK number of data: 1532
Dataset 5 TCIA LIDC: OK number of data: 450
Dataset all training: number of data: 4500
loader is ready
Dataset 1 LUNA16: OK number of data: 842
Dataset 2 Covid 19: OK number of data: 722
Dataset 3 HNSCC: OK number of data: 954
Dataset 4 TCIA Colon: OK number of data: 1532
Dataset 5 TCIA LIDC: OK number of data: 450
Dataset all training: number of data: 4500
loader is ready
start proj
torch.Size([8, 78, 96, 96, 96])
torch.Size([8, 400, 96, 96, 96])
torch.Size([8, 96, 96, 400, 96])
start proj
torch.Size([8, 78, 96, 96, 96])
torch.Size([8, 400, 96, 96, 96])
torch.Size([8, 96, 96, 400, 96])
Traceback (most recent call last):
  File "/home/karimimonsefi.1/SSL_VAN/pretrain/main.py", line 214, in <module>
    main()
  File "/home/karimimonsefi.1/SSL_VAN/pretrain/main.py", line 111, in main
    mp.spawn(main_worker, nprocs=args.ngpus_per_node, args=(args,))
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 1 terminated with the following error:
Traceback (most recent call last):
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/karimimonsefi.1/SSL_VAN/pretrain/main.py", line 196, in main_worker
    loss_value = run_training(
  File "/home/karimimonsefi.1/SSL_VAN/pretrain/training.py", line 121, in run_training
    train_loss = train_epoch(
  File "/home/karimimonsefi.1/SSL_VAN/pretrain/training.py", line 55, in train_epoch
    logits = model(data)
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/karimimonsefi.1/SSL_VAN/commons/models/pre_training/pre_van_v4.py", line 98, in forward
    x = self.pre_train_proj(x)
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/karimimonsefi.1/SSL_VAN/commons/models/pre_training/pretrain_projection.py", line 34, in forward
    x = self.conv2(x)
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 613, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 608, in _conv_forward
    return F.conv3d(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.96 GiB (GPU 1; 39.42 GiB total capacity; 29.40 GiB already allocated; 5.48 GiB free; 32.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

