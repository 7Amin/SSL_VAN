/var/spool/slurmd/job33429/slurm_script: line 10: cd: SSL_VAN: No such file or directory

----------------------------------------------------------------------------
  cuda:
----------------------------------------------------------------------------
     Versions:
        cuda/10.2
        cuda/11.0
        cuda/11.2
        cuda/11.3
        cuda/11.4
        cuda/11.5
        cuda/11.6
        cuda/12.0

----------------------------------------------------------------------------
  For detailed information about a specific "cuda" package (including how to load the modules) use the module's full name.
  Note that names that have a trailing (E) are extensions provided by other modules.
  For example:

     $ module spider cuda/12.0
----------------------------------------------------------------------------

 


CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
/home/karimimonsefi.1/SSL_VAN/BTCV/main.py:119: UserWarning: 0 gpu 0
  warnings.warn(f"{args.rank} gpu {args.gpu}")
/home/karimimonsefi.1/SSL_VAN/BTCV/main.py:121: UserWarning: Batch size is: 4 epochs 5000
  warnings.warn(f"Batch size is: {args.batch_size} epochs {args.max_epochs}")
/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/monai/transforms/post/array.py:176: UserWarning: `to_onehot=True/False` is deprecated, please use `to_onehot=num_classes` instead.
  warnings.warn("`to_onehot=True/False` is deprecated, please use `to_onehot=num_classes` instead.")
/home/karimimonsefi.1/SSL_VAN/BTCV/main.py:192: UserWarning: Total parameters count 23127538
  warnings.warn(f"Total parameters count {pytorch_total_params}")
/home/karimimonsefi.1/SSL_VAN/BTCV/main.py:196: UserWarning: Total args.checkpoint False
  warnings.warn(f"Total args.checkpoint {args.checkpoint}")
/home/karimimonsefi.1/SSL_VAN/BTCV/trainer.py:138: UserWarning: Writing Tensorboard logs to ./runs/BTCV/test_log
  warnings.warn(f"Writing Tensorboard logs to {args.logdir}")
/home/karimimonsefi.1/SSL_VAN/BTCV/trainer.py:147: UserWarning: 0  Fri Mar 24 07:49:28 2023  Epoch: 0
  warnings.warn(f"{args.rank}  {time.ctime()}  Epoch: {epoch}")
input_list/dataset_BTCV_List.json
Traceback (most recent call last):
  File "/home/karimimonsefi.1/SSL_VAN/BTCV/main.py", line 270, in <module>
    main()
  File "/home/karimimonsefi.1/SSL_VAN/BTCV/main.py", line 102, in main
    main_worker(gpu=0, args=args)
  File "/home/karimimonsefi.1/SSL_VAN/BTCV/main.py", line 249, in main_worker
    accuracy = run_training(
  File "/home/karimimonsefi.1/SSL_VAN/BTCV/trainer.py", line 149, in run_training
    train_loss = train_epoch(
  File "/home/karimimonsefi.1/SSL_VAN/BTCV/trainer.py", line 43, in train_epoch
    loss.backward()
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/home/karimimonsefi.1/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.21 GiB (GPU 0; 39.42 GiB total capacity; 36.84 GiB already allocated; 843.00 MiB free; 37.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
